
\subsection*{Related Papers \& Architectures}
 This paper is based on 3 documents that serve as a starting 
 point and tools for the proposed model:
 
\begin{itemize}[noitemsep]
    \item \texttt{(Farhadi et al., 2009; Kumar et al., 2009;
    Parikh \& Grauman, 2011; Lampert et al., 2014)}: 
    3 Useful Papers for Encoding Distinctive Features of Objects 
    into Vectors (such as attributes used to distinguish 
    between different classes of objects)
    \item \texttt{(Fu et al., 2014 ; Akata et al., 2015)}: 
    2 Papers on "zero-shot" recognition, that is, recognizing 
    objects that have never been seen during the model's training.
    \item \texttt{(Yan et al., 2015).}: 
    And in Yan's paper, they discuss conditional image generation 
    in a manner similar to the method proposed here.
    \item \texttt{ (Reed et al., 2016)}: 
    Reed's paper presents highly discriminative and generic
    "zero-shot" text representations, 
    which are learnt automatically from words and characters.
    \item \texttt{(Goodfellow et al., 2014) \& (also studied
    by Mirza \& Osindero (2014) and Denton et al. (2015) )}: 
    Paper on application of conditional multi-modality for generative adversarial networks 

    \item \texttt{LEARNING A SHARED RAPPRESENTAZIONE ACROSS MODALITIES (MULTI-MODAL):}

    \item \texttt{Ngiam et al. (2011)}: 
    trained a stacked multimodal autoencoder on audio and video inputs and achieved a shared 
    modality-invariant representation.

    \item \texttt{Srivastava \& Salakhutdinov (2012)}: 
    developed a deep Boltzmann machine and jointly modeled images and text tags

    \item \texttt{Sohn et al. (2014) }
    proposed a multimodal conditional prediction framework

    \item \texttt{DEEP CONVOLUTIONAL DECODER NETWORK ARCHITECTURE :}
    
    \item \texttt{Dosovitskiy et al. (2015) \& Yang et al. (2015)}
    Used a deconvolutional network (with many layers of convolution and upsampling) 
    to create 3D chair representations based on shape, location, and illumination.
    And Yan added an encoder network to this approach .
    They trained a recurrent neural encoder-decoder to rotate 3D chair models
    and human faces based on rotational action sequences.

    \item \texttt{Reed et al. (2015)}
    Used a convolutional decoder to predict visual parallels between forms, video game characters, and 3D automobiles.

    \item \texttt{Goodfellow et al. (2014)}
    Introduced generative adversarial networks (GANs) and showed how GANs benefit from convolutional decoder networks for the generator module.

    \item \texttt{GENERATIVE ADVERSIAL NETWORK ....}
    
    \item \texttt{Denton et al. (2015)}
    Used a Laplacian pyramid of adversarial generators and discriminators to synthesize images 
    at multiple resolutions, producing high-resolution images and allowing generation conditioned on class labels

    \item \texttt{Radford et al. (2016)}
    Developed a stable and effective GAN architecture using a standard convolutional decoder, 
    incorporating batch normalization to improve image synthesis quality.

    \item \texttt{MULTI-MODAL LEARNING  }


    \item \texttt{Vinyals et al. (2015), Mao et al. (2015), Karpathy \& Li (2015), Donahue et al. (2015):}
    Introduced the use of recurrent neural network (RNN) decoders to generate text descriptions conditioned on images.

    \item \texttt{Hochreiter \& Schmidhuber (1997): }
    Used Long Short-Term Memory (LSTM) networks conditioned on top-layer features of deep convolutional networks 
    to generate image captions, especially with datasets like MS COCO.

    \item \texttt{Xu et al. (2015): }
    Incorporated a recurrent visual attention mechanism to further improve the results of text generation from images.

    \item \texttt{Ren et al. (2015) :}
    Generated replies to inquiries concerning picture visual content.

    \item \texttt{Wang et al. (2015) :}
    Expanded on Ren et al.'s technique by including an explicit knowledge foundation in the replying process.

    \item \texttt{Zhu et al. (2015) :}
    Used sequence models to align text (from books) and movies, allowing for simultaneous alignment of both.

    \item \texttt{Mansimov et al. (2016) :}
    Created pictures from text captions using a variational recurrent autoencoder with attention, 
    comparable to the DRAW model, which synthesized images in many stages.

    \item \texttt{Gregor et al. (2015) :}
    Created the DRAW model, which influenced Mansimov's way of producing graphics step by step.
    And obtaining realistic result also with "zero-shot" descriptions showing generalization . 

    \item \texttt{}
    \item \texttt{}


\end{itemize}